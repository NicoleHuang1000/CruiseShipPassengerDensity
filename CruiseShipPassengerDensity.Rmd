---
title: "Principal-Component-Regression-Analysis"
author: 'Jiemin Huang'
output:
  pdf_document:
    df_print: paged
  html_document:
    df_print: paged
---





\newpage

## Questions

1. **(8 marks)**

    Data were collected on 158 cruise ships in operation around the world in 2013.  Complaints had been raised by customers about overcrowding on cruises and there was interest in investigating whether there was a trend of overcrowding on certain types of ships.  As part of the investigation, a regression analysis was carried out that could be used to predict passenger density (number of passengers per unit area) based on ship characteristics.  The data are available in the dataset `cruise_ship.csv` and include the following variables:

    Variable | Description
    --- | ------------------
    `age.2013` | Age (as of 2013) 
    `tonnage` | Weight of ship (1000s of tonnes)
    `passengers.100` | Maximum number of passengers (100s)
    `length` | Length of ship (100s of feet)
    `cabins` | No. of passenger cabins (100s) 
    `crew.100` | No. of crew member (100s) 
    `pass.density` | Passenger density (no. of passengers per square foot)

    There were high correlations among some pairs of predictors. As a result, principal components regression was used to avoid potential multi-collinearity issues.
    a. **(3 marks)** Obtain the principal components (PCs) for the 6 predictors in the dataset using the `pcr` command and provide summary output.  Based on the output:
        i. How many principal components are required to explain at least 90% of the variance in the predictors?
 
```{r}
if (!require("pls", character.only = TRUE)) {
  install.packages('pls')
} 
```
```{r}
if (!require("caret", character.only = TRUE)) {
  install.packages('caret')
}
```
```{r}
cruise <- read.csv('cruise_ship.csv')
cruise <- cruise[,4:10]
str(cruise)
```

```{r}
library(pls)

set.seed(1)
pcr_cruise <- pcr(pass.density ~., data = cruise, scale = TRUE)
summary(pcr_cruise)
```

According to the results, at least 2 components are required if we want to have at least 90% of the variance in the predictors.

        ii. What would be the $R^2$ value for a linear model for `pass.density` that includes all predictors.  Explain your answer based on the summary output of the `pcr` command. 
  
        
```{r}
fit_ln <- lm( pass.density ~ ., data = cruise)
library(pander)
pander(summary(fit_ln), caption = "")
```

In the output of the linear model,$R^2 = $ 0.6709, which means 67.09% variance in the response variable that is explained by the predictors in the model. 

In the output of 'pcr' command, 6 principal components explains 67.09% of the variance in the response variable.

Both methods include all predictors, they capturing the same underlying patterns in the data here.It's important to note that this may not always be the case, and the effectiveness of each method can vary depending on the specific characteristics of the data.

    b. **(3 marks)** Obtain a plot of the cross-validation mean squared error (MSE) for each number of principal components and state the number of PCs that gives the smallest cross-validation MSE.  
    

```{r}
pcr_cv <- pcr(pass.density ~., data = cruise, scale = TRUE, validation = "CV")
summary(pcr_cv)
validationplot(pcr_cv, val.type = "MSEP")
min.pcr = which.min(MSEP(pcr_cv)$val[1,1,]) - 1
min.pcr
```

When PCs = 5, the smallest MSE is reached. 

    c. **(2 marks)** Based on your results from parts (a) and (b), how many PCs would you choose to represent the predictors in a regression model in place of all the predictors. Explain your answer briefly.

Using principal components (PCs) in combination with cross-validation in (b) can help us to select the optimal number of components. 

According to (a) and (b), when considering 5 PCs, the predictors explain 99.73% of the variance,which is close to the results for 4 PCs and 6 PCs. While the response variable of 5 PCs explains 66.20%, which is much higher than that of 4 PCs and quite colse to the result of 6 PCs. The cross-validation MSE is lowest when considering 5 PCs. By choosing 5 PCs, we can effectively reduce the dimensionality of the predictors while maintaining good predictive performance.

Thus, I would choose 5 principal components.

2. **(12 marks)**
    
    In a 1978 study on absenteesim from school, data on 146 children from Walgett, New South Wales, Australia were collected.  The number of days absent from school in a particular school year was recorded for each child, together with some demographic information.  The data are available in the dataset `quine.csv` and include the following variables:  

    Variable | Description
    ----- | ------------------
    `Eth |` Ethnic background | Aboriginal or Not, ("A" or "N").
    `Sex |` Factor with levels ("F" or "M").
    `Age |` Primary ("F0"), or forms "F1," "F2" or "F3". 
    `Lrn |` "AL" = Average learner, "LD"=Learning disabilities.
    `Days |` Days absent from school in the year. 

    a. **(2 marks)** Explain briefly why it is unnecessary to use an offset variable in a model for the number of days absent. 
    
An offset variable is typically used when there is a known exposure or risk factor that needs to be taken into account in the analysis, but it is not the variable of interest itself. Here, the number of days absent is already the variable of interest and does not need to be adjusted or offset by any other variable.
    
    b. **(3 marks)** Fit a Poisson and a negative binomial regression model with `Days` as the response variable and the rest of the variables as predictors.  Obtain plots of residuals against predicted values.  Comment on what the plots show. 
    
```{r}
library(stats) 
library(MASS)   

quine <- read.csv("quine.csv")

quine_pois <- glm(Days ~ as.factor(Eth) + as.factor(Sex) + as.factor(Age) + as.factor(Lrn), data = quine, family = poisson)
quine_nb <- glm.nb(Days ~ as.factor(Eth) + as.factor(Sex) + as.factor(Age) + as.factor(Lrn), data = quine)

par(mfrow = c(1,2))

plot(predict(quine_pois, type = "response"), residuals(quine_pois), xlab = "Predicted Values", ylab = "Residuals", main = "Poisson Regression", ylim = c(-5, 15))
abline(h = 0, lty = 1, col = 'red')
lines(lowess(predict(quine_pois, type = "response"), residuals(quine_pois)), lwd = 2, lty = 2)

plot(predict(quine_nb, type = "response"), residuals(quine_nb), xlab = "Predicted Values", ylab = "Residuals", main = "Negative Binomial Regression", ylim = c(-5, 15))
abline(h = 0, lty = 1, col = 'red')
lines(lowess(predict(quine_nb, type = "response"), residuals(quine_nb)), lwd = 2, lty = 2)
```

The dotted line for the poisson regression model lies below the red line for lower predicted values and lies above the red line for higher predicted values. This pattern indicates that the variance of the residuals is not constant across different levels of predicted values.

The dotted line for the negative binomial model shows no trend in the residual vs predicted values relationship.

    c. **(2 marks)** Calculate AIC and BIC statistics for both models in part (b) and print these in a table. State the preferred model based on these results. 
    
```{r}
ICs <- data.frame(c('Poisson', 'NB'),
                  c(AIC(quine_pois), AIC(quine_nb)),
                  c(BIC(quine_pois), BIC(quine_nb)))
colnames(ICs) <- c('Model', 'AIC', 'BIC')
library(pander)
pander(ICs)
```
AIC and BIC both indicate preference for the negative binomial model.


    d. **(2 marks)** Give the value of $\hat\theta$ from the fitted negative binomial model.
    
```{r}
summary(quine_nb)
```
$\hat\theta = 1.2749$

    e. **(3 marks)** Use the formula for $Var(Y)$ for the negative binomial distribution and the value of $\hat\theta$ in part (d) to explain  why your conclusion in part (c) is not surprising.
  
```{r}
theta <- summary(quine_nb)$theta
mean_Y <- mean(quine$Days)
var_Y <- mean_Y + (mean_Y^2) / theta
print(var_Y)
```

Var(Y) >> E(Y), this is over-dispersion. When overdispersion is present, Poisson regression may not adequately capture the variability in the data, but Negative binomial regression is a more flexible model than Poisson regression and can accommodate overdispersion. By allowing for overdispersion, the negative binomial model can better capture the variability present in the data and provide more accurate predictions. 

3. **(20 marks)**

    The majority of modern high rise structures are dependent on concrete for structural integrity and durability.  High-performance concrete (HPC) is made using a mix of ingredients, and it is of interest to predict the performance of HPC from the composition of ingredients.  We will make use of data contained in the dataset `concrete.csv`.  This dataset contains experimental data on the use of seven different ingredients in HPC:

    * cement (`CEMENT`)
    * slag (`SLAG`)
    * fly ash (`FLY_ASH`)
    * water (`WATER`)
    * superplasticizer (`SP`)
    * coarse aggregate (`COARSE_AG`)
    * fine aggregate (`FINE_AG`)

    All of these ingredients are measured in kg/m$^3$.  Three different outcomes were measured:

    * slump (`SLUMP`)
    * flow (`FLOW`)
    * 28-day compressive strength (`COMP_STRENGTH`)
    
    a. **(1 mark)** It is of interest to estimate the coefficient of variation for the slump-to-flow ratio $\left(\frac{\tt SLUMP}{\tt FLOW}\right)$.  Note that the coefficient of variation is given by
        \begin{eqnarray*}
          \mbox{CV} &=& \frac{\sigma}{\mu},
        \end{eqnarray*}
        which is estimated by
        \begin{eqnarray*}
          \widehat{\mbox{CV}} &=& \frac{S}{\overline{X}}.
        \end{eqnarray*}
        Estimate the coefficient of variation for the slump-to-flow ratio from the original data.
        
```{r}
concrete <- read.csv('concrete.csv')
concrete <- concrete[,2:11]  # delete the first column "ID"
str(concrete)
```

```{r}
concrete['s_f_ratio'] <- concrete['SLUMP'] / concrete['FLOW']
s_f_mean <- mean(concrete$s_f_ratio)
s_f_std <- sd(concrete$s_f_ratio)
original.cv <- s_f_std / s_f_mean
original.cv
```

    b. **(3 marks)** Now use 10,000 bootstrap samples to simulate the sampling distribution for the coefficient of variation for the slump-to-flow ratio.  Present a density plot of the sampling distribution and a vertical bar at the estimated coefficient of variation from the original data.  Describe the shape of the sampling distribution.

    We now turn our focus to predicting `COMP_STRENGTH`, measured in millions of pascals (MPa), using a linear regression of `COMP_STRENGTH` on the seven ingredients: `CEMENT`, `SLAG`, `FLY_ASH`, `WATER`, `SP`, `COARSE_AG`, and `FINE_AG`.
    `CEMENT`)
    * slag (`SLAG`)
    * fly ash (`FLY_ASH`)
    * water (`WATER`)
    * superplasticizer (`SP`)
    * coarse aggregate (`COARSE_AG`)
    * fine aggregate (`FINE_AG`)

```{r}
library(caret)
library(ggplot2)
set.seed(0)
nboot <- 10000
indices <- 2:7

bootstrap.samples <- createResample(concrete$s_f_ratio, times = nboot, list = TRUE)

bs.cv <- function(indices){
  sample <- concrete$s_f_ratio[indices]
  sd(sample)/mean(sample)
}
bootstrap.cv <- sapply(bootstrap.samples, FUN = bs.cv)

ggplot(data = data.frame(bootstrap.cv), aes(x = bootstrap.cv)) + 
  geom_density(alpha = 0.5) + geom_vline(xintercept = original.cv, color = "red", linetype = "dashed", linewidth = 1) + labs(title = "Density Plot of Bootstrap CVs for Slump-to-Flow Ratio",
  x = "Coefficient of variation(CV)", y = "Density")

```
```{r}
comp_ln <- lm(COMP_STRENGTH ~ CEMENT + SLAG + FLY_ASH + WATER + SP + COARSE_AG + FINE_AG, data = concrete)
#summary(comp_ln)
```


    c. **(4 marks)** Carry out an exhaustive model search using best subset selection of the seven predictors `CEMENT`, `SLAG`, `FLY_ASH`, `WATER`, `SP`, `COARSE_AG`, and `FINE_AG`.  Use the `regsubsets` function in the `leaps` package, explaining which predictors would be included in the best model selected using BIC.  How would the set of predictors selected change if instead using adjusted $R^{2}$ or the $C_{p}$ statistic?
    
```{r}
if (!require("leaps", character.only = TRUE)) {
  install.packages('leaps')
}
library(leaps)
all_models <- regsubsets(COMP_STRENGTH ~ CEMENT + SLAG + FLY_ASH + WATER + SP + COARSE_AG + FINE_AG, data = concrete, method = "exhaustive")

#predictors included in the best model selected using BIC
best_bic_model <- which.min(summary(all_models)$bic)
best_bic_model_vars <- names(coef(all_models, best_bic_model))

#predictors included in the best model selected using R-squared
best_adjr2_model <- which.max(summary(all_models)$adjr2)
best_adjr2_model_vars <- names(coef(all_models, best_adjr2_model))

##predictors included in the best model selected using Cp statistic
best_cp_model <- which.min(summary(all_models)$cp)
best_cp_model_vars <- names(coef(all_models, best_cp_model))

cat("Best model selected using BIC: ", best_bic_model_vars, "\n")
cat("Best model selected using R^2: ", best_adjr2_model_vars, "\n")
cat("Best model selected using Cp statistic: ", best_cp_model_vars, "\n")

```
The regsubsets function generates all possible combinations of predictors from the provided predictor variables. For each combination of predictors, the function fits a linear regression model. Then it calculates the relevant goodness-of-fit statistics such as BIC, adjusted $R^{2}$ or the $C_{p}$ statistic. Based on BIC and $C_{p}$ statistic, the model with minimumBIC or $C_{p}$ value is the best model. Based on adjusted $R^{2}$, the model with the largest adjusted $R^{2}$ value is the best model. The set of predictors selected changes when using different criterion. 


    d. **(4 marks)** Now carry out best subset selection using 20 repetitions of 10-fold cross-validation and the criterion of test MSE.  Focussing on the top 10 models in terms of test MSE, which predictors would be included in the best model selected using this approach?  Why?
    
```{r}
library(caret)
library(doParallel)
library(foreach)
library(pander)

variable.indices <- 1 : 7 

# Produce a matrix that represents all possible combinations of predictors.
all.comb <- expand.grid(as.data.frame(matrix(rep(0 : 1, length(variable.indices)), nrow = 2)))[-1,]

# Fire up 75% of computer cores for parallel processing.
nclust <- makeCluster(detectCores() * 0.75)
registerDoParallel(nclust)

# MSE: Repeated 10-fold cross-validation. 
set.seed(0)
folds <- 10
reps <- 20

fitControl <- trainControl(method = "repeatedcv", number = folds, repeats = reps, seeds = 1 :
(folds * reps + 1), savePredictions = TRUE)

model.fits <- foreach(i = 1 : nrow(all.comb), .packages = "caret") %dopar%
{
model.equation <- as.formula(paste("COMP_STRENGTH ~", paste(names(concrete)[variable.indices][all.comb[i,] == 1], collapse = " + ")))
train(model.equation, data = concrete, method = "lm", trControl = fitControl)
}

# Shut down cores.
stopCluster(nclust)

# Write a custom function that will extract RMSE for each candidate model
# and square it to produce the estimate of MSE.
MSE.extract <- function(x)
{
return(as.numeric(x$results[2]) ^ 2)
}

# Apply the function to all of the candidate models that were fit using
# repeated 10-fold cross-validation.
MSE.rep.cv <- sapply(model.fits, MSE.extract)

# View the 10 lowest estimated values for test MSE.
sort(MSE.rep.cv)[1 : 10]
```

```{r}
# View the top 10 models in terms of the objective of minimising MSE.  
order(MSE.rep.cv)[1 : 10]
```

```{r}
# Construct a matrix in which to store information on which variables are 
#included in the 10 best models.
best.models <- matrix(NA, nrow = 10, ncol = length(variable.indices), dimnames = list(NULL,
names(concrete)[variable.indices]))

# Cycle through the top 10 models and save TRUEs and FALSEs for columns of variables included 
#and not included in the best models.
for(i in 1 : 10)
{
best.models[i, ] <- all.comb[order(MSE.rep.cv)[i], ] == 1
}

pander(best.models)
```

The first model with 6 predictors, 'CEMENT', 'FLY_ASH', 'WATER', 'SP', 'COARSE_AG' and 'FINE_AG' is the best model selected here.

This model was chosen because it achieved the lowest test MSE among the top 10 models considered. Lower test MSE indicates better predictive performance on unseen data. By selecting this model, we aim to achieve good performance on new, unseen data in the future. 

    e. **(3 marks)** Finally, use 20 repetitions of 10-fold cross-validation to find the optimal number of components if using principal component regression to predict the compression strength.
    
```{r}
set.seed(0)
reps <- 20
folds <- 10
unregister_dopar <- function()
{
env <- foreach:::.foreachGlobals
rm(list=ls(name=env), pos=env)
}

unregister_dopar()
variable.indices <- 1:7

# Carry out repeated k-fold cross-validation of PCR.
fitControl <- trainControl(method = "repeatedcv", number = folds, repeats = reps,
savePredictions = TRUE)
model.equation <- as.formula(paste("COMP_STRENGTH ~", paste(names(concrete)[variable.indices],
collapse = " + ")))
pcr <- train(model.equation, data = concrete, method = "pcr", scale = TRUE, tuneLength = 10,
trControl = fitControl)

# Produce a plot of estimated RMSE vs. number of PCs.
plot(pcr)
```
```{r}
pander(cbind("PCs" = 1 : 6, "MSE" = as.numeric(pcr$results[, 2] ^ 2)))
```
```{r}
summary(pcr$finalModel)
```
According to the results the optimal number of components if using principal component regression to predict the compression strength is 6. Because with 6 components, the model reaches the lowest test MSE, which is 6.948. The predictors with 6 components explain 99.96% of the variability and the response variable explains 89.66% of the variability, both indicating robust explanatory power.

    f. **(5 marks)** Compare the three different methods used for model selection in parts (c), (d), and (e):
    
        * best subset selection using an exhaustive model search using `regsubsets` and the criteria of adjusted $R^{2}$, the $C_{p}$ statistics, and/or BIC;
        * best subset selection using an exhaustive model search using 20 repetitions of 10-fold cross-validation and the criterion of test MSE; and
        * principal component regression using 20 repetitions of 10-fold cross-validation and the criterion of test MSE.
        
        Briefly explain the relative advantages and disadvantages of the three methods and when we would prefer that particular method.  Explain which method's results you would likely prefer here and why?
        
(i) best subset selection using an exhaustive model:

(1) advantages: Provides a comprehensive exploration of all possible combinations of predictors, allowing for the selection of the best subset based on criteria like adjusted $R^{2}$, $C_{p}$ statistics, or BIC.

(2) disadvantages: Can be computationally expensive, especially with larger numbers of predictors. Prone to overfitting if the number of predictors is large relative to the sample size.

preference: This method is suitable when the number of predictors is relatively small, and a precise understanding of the relationship between predictors and response is desired.

(ii) Best subset selection using cross-validation:

(1) advantages: Provides a more robust estimate of model performance by assessing generalization ability through cross-validation. This method helps mitigate overfitting.

(2) disadvantages: It requires an exhaustive search over possible subsets, which can be computationally intensive. When the number of predictors is large relative to the sample size, this method is not efficient.

preference: This method is preferable when computational resources are limited and a balance between model performance and computational complexity is sought.

(iii) Principal component regression (PCR) using cross-validation:

(1) advantages: Reduces dimensionality by transforming predictors into a smaller set of uncorrelated components, potentially improving model interpretability and reducing overfitting. Cross-validation provides robust estimation of model performance.

(2) disadvantages: PCR assumes linearity between predictors and response, which may not always hold. Loss of interpretability of individual predictors.

preference: PCR is suitable when dealing with a large number of correlated predictors or when interpretability of individual predictors is not the primary concern. Cross-validation ensures reliable estimation of model performance.

Given the scenario here, the goal is to predict compression strength. The number of predictors is relatively small, thus we do not worry about computation expense. PCR with cross-validation method is not my choice due to limited number of predictors. A precise understanding of the relationship between predictors and response is desired here. I think the best subset selection using an exhaustive model search in (c) based on adjusted $R^{2}$, $C_{p}$ statistics, or BIC might be preferable. This method allows for a comprehensive exploration of all possible combinations of predictors and facilitates the identification of the most relevant predictors while maintaining interpretability.

**Assignment total: 40 marks**
